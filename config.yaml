# Section to configure all the models and providers for generation
models:
  generator_provider : anthropic # The main provider to use to generate the data
  default_models: # Models called for generation. Dependent on generator_provider value. E.g: provider 'mistral' will use generator_mistral model
    generator_anthropic : claude-haiku-4-5-20251001
    generator_mistral : mistral-small-2506

# Options to control data generation
generation:
  max_len : 300 # Max tokens used for output
  total_stories : 100 # Max stories to produce in total
  batch_size : 100 # Size of the batch
  prompt: cooperation-en # Name of the prompt used to generate synthetic data. As in src/prompts.py
  story_setups: # Indicates which setups to use for the stories. As in src/setups.py. If 'all' it will use all in src/setups.py
    - all

# Options to control evaluation
evaluation:
  judge_model : anthropic:/claude-sonnet-4-5 # Model used as LLM-as-a-judge for data evaluation
  run_name : mistral_evaluation # Name of the run name for mlflow
  metrics: # Indicates the metrics to use to evaluate the data. As in src/metrics.py. If 'all' it will use all in src/metrics.py
  - is_gramatically_correct
  - is_understandable

# Indicates the files required for the different scripts
data:
  vocabulary: data/vocabulary.json # File with verbs, nouns and adjectives to use to forest diversification
  prompt_templates : data/templates.jsonl # The file where the input prompts will be stored for posterior generation. Used in generate_prompts.py
  output: data/stories_mistral_v2.jsonl # File where synthetic data will be stored. Used by main.py
  batch_input: data/batch_input.jsonl # File where batch data will be held prior to generation. Currently supported only for Mistral format. Used in clients.py

